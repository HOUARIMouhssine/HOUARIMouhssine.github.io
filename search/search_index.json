{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"dns/","text":"Welcome to MkDocs::DNS Pour toute la documentation, rendre visite \u00e0 mkdocs.org . Explication fichier /etc/named.conf options { listen-on port 53 { 127.0.0.1 ; 192.168.0.1; }; directory \"/var/named\"; pid-file \"/run/named/named.pid\"; dump-file \"/var/named/data/cache_dump.db\"; statistics-file \"/var/named/data/named_stats.txt\"; memstatistics-file \"/var/named/data/named_mem_stats.txt\"; //allow-query { localnets ; }; allow-query { any; }; forwarders { 8.8.8.8 ; 8.8.4.4 ; }; }; zone \"ahouari\" { type master; file \"/var/named/ahouari.db\"; }; zone \"0.168.192.in-addr.arpa\" { type master; file \"/var/named/0.168.192.db\"; }; listen-on port 53 : cela, veut dire que notre serveur de bind \u00e9coute en localhost et sur l'adresse 192.168.0.1 sur le port 53 dump-file : Ce fichier, explique l'emplacement dont le DNS serveur va stocker le cache de son contenue statistics-file : Ce fichier, repr\u00e9sente les informations statistiques stocker par le serveur bind pour ses op\u00e9rations memstatistics-file : Repr\u00e9sente les informations statistiques reli\u00e9 \u00e0 la m\u00e9moire (RAM) allow-query : repr\u00e9sente une/plusieurs adresses IP qui ont le droit pour lancer des requetes pour ce DNS serveur zone \"ahouari\" : cr\u00e9ation d'une zone nomm\u00e9e ahouari et que zone est master et son fichier est dans /var/named/ahouari.db zone \"0.168.192.in-addr.arpa\" : Cette partie est la zone reverse PTR de la zone ahouari pour l'adresse r\u00e9seau 192.168.0.0, c'est indiqu\u00e9 que cette zone est la zone reverse de la zone master ahouari et son fichier de configuration est localis\u00e9 sur /var/named/0.168.192.in-addr-arpa Cette configuration d\u00e9finit la partie zone DNS sur un r\u00e9seau qui \u00e9coute dans le port 53 TCP/UDP, ce serveur autorise les queries DNS depuis n'importe quel serveur et il les forwardes vers les DNS de google les autres machines dans le r\u00e9seau peuvent sp\u00e9cifier l'adresse ip de ce dns serveur afin de faire des requ\u00eates DNS Explication de fichier **/var/named/ahouari.db $ORIGIN . $TTL 10800 ; 3 hours ahouari IN SOA ah200.ahouari. admin.ah200.ahouari. ( 2013022757 ; serial 3600 ; refresh (1 hour) 3600 ; retry (1 hour) 604800 ; expire (1 week) 38400 ; minimum (10 hours 40 minutes) ) NS ah200.ahouari. $ORIGIN ahouari. ah200 A 192.168.0.1 ah201 A 192.168.0.2 ah100 A 192.168.0.10 ah101 A 192.168.0.11 ah102 A 192.168.0.12 Explication de fichier ** /var/named/0.168.192.db ** ```","title":"DNS"},{"location":"dns/#welcome-to-mkdocsdns","text":"Pour toute la documentation, rendre visite \u00e0 mkdocs.org .","title":"Welcome to MkDocs::DNS"},{"location":"dns/#explication-fichier-etcnamedconf","text":"options { listen-on port 53 { 127.0.0.1 ; 192.168.0.1; }; directory \"/var/named\"; pid-file \"/run/named/named.pid\"; dump-file \"/var/named/data/cache_dump.db\"; statistics-file \"/var/named/data/named_stats.txt\"; memstatistics-file \"/var/named/data/named_mem_stats.txt\"; //allow-query { localnets ; }; allow-query { any; }; forwarders { 8.8.8.8 ; 8.8.4.4 ; }; }; zone \"ahouari\" { type master; file \"/var/named/ahouari.db\"; }; zone \"0.168.192.in-addr.arpa\" { type master; file \"/var/named/0.168.192.db\"; }; listen-on port 53 : cela, veut dire que notre serveur de bind \u00e9coute en localhost et sur l'adresse 192.168.0.1 sur le port 53 dump-file : Ce fichier, explique l'emplacement dont le DNS serveur va stocker le cache de son contenue statistics-file : Ce fichier, repr\u00e9sente les informations statistiques stocker par le serveur bind pour ses op\u00e9rations memstatistics-file : Repr\u00e9sente les informations statistiques reli\u00e9 \u00e0 la m\u00e9moire (RAM) allow-query : repr\u00e9sente une/plusieurs adresses IP qui ont le droit pour lancer des requetes pour ce DNS serveur zone \"ahouari\" : cr\u00e9ation d'une zone nomm\u00e9e ahouari et que zone est master et son fichier est dans /var/named/ahouari.db zone \"0.168.192.in-addr.arpa\" : Cette partie est la zone reverse PTR de la zone ahouari pour l'adresse r\u00e9seau 192.168.0.0, c'est indiqu\u00e9 que cette zone est la zone reverse de la zone master ahouari et son fichier de configuration est localis\u00e9 sur /var/named/0.168.192.in-addr-arpa Cette configuration d\u00e9finit la partie zone DNS sur un r\u00e9seau qui \u00e9coute dans le port 53 TCP/UDP, ce serveur autorise les queries DNS depuis n'importe quel serveur et il les forwardes vers les DNS de google les autres machines dans le r\u00e9seau peuvent sp\u00e9cifier l'adresse ip de ce dns serveur afin de faire des requ\u00eates DNS","title":"Explication fichier /etc/named.conf"},{"location":"dns/#explication-de-fichier-varnamedahouaridb","text":"$ORIGIN . $TTL 10800 ; 3 hours ahouari IN SOA ah200.ahouari. admin.ah200.ahouari. ( 2013022757 ; serial 3600 ; refresh (1 hour) 3600 ; retry (1 hour) 604800 ; expire (1 week) 38400 ; minimum (10 hours 40 minutes) ) NS ah200.ahouari. $ORIGIN ahouari. ah200 A 192.168.0.1 ah201 A 192.168.0.2 ah100 A 192.168.0.10 ah101 A 192.168.0.11 ah102 A 192.168.0.12","title":"Explication de fichier **/var/named/ahouari.db"},{"location":"dns/#explication-de-fichier-varnamed0168192db","text":"```","title":"Explication de fichier ** /var/named/0.168.192.db **"},{"location":"doc/","text":"Welcome to MkDocs::Documentation Pour toute la documentation, rendre visite \u00e0 mkdocs.org . Source de l'environnement source /data/python/bin/activate Entrer dans le r\u00e9pertoire cd /data/Documentation git fetch git init git add . mkdocs gh-deploy -r https://github.com/HOUARIMouhssine/ahouari.github.io -b main Cr\u00e9ation d'un service Pour que l'url du doc soit accessible en temps r\u00e9el ( ce qui veut dire sans sourcer l'environnement \u00e0 chaque fois et lancer le mkdocs serve), j'ai cr\u00e9e un fichier bash d\u00e9dier pour faire \u00e7a. [root@ah200 Documentation]# cat permanent_running.sh #!/bin/bash # Source your virtual environment source /data/python/bin/activate # Run mkdocs serve cd /data/Documentation && mkdocs serve Pour que cette partie soit permanente, j'ai cr\u00e9e un fichier daemon sous forme du service : [root@ah200 Documentation]# systemctl status mkdocs.service \u25cf mkdocs.service - MKDocs Service Loaded: loaded (/etc/systemd/system/mkdocs.service; disabled; preset: disabled) Active: active (running) since Mon 2024-02-12 16:50:31 UTC; 6 days ago Main PID: 902882 (permanent_runni) Tasks: 8 (limit: 203086) Memory: 33.5M CPU: 12min 40.005s CGroup: /system.slice/mkdocs.service \u251c\u2500902882 /bin/bash /data/Documentation/permanent_running.sh \u2514\u2500902883 /data/python/bin/python /data/python/bin/mkdocs serve [root@ah200 Documentation]# cat /etc/systemd/system/mkdocs.service [Unit] Description=MKDocs Service After=network.target [Service] Type=simple ExecStart=/bin/bash -c '/data/Documentation/permanent_running.sh' [Install] WantedBy=multi-user.target [root@ah200 Documentation]#","title":"Documentation"},{"location":"doc/#welcome-to-mkdocsdocumentation","text":"Pour toute la documentation, rendre visite \u00e0 mkdocs.org .","title":"Welcome to MkDocs::Documentation"},{"location":"doc/#source-de-lenvironnement","text":"source /data/python/bin/activate","title":"Source de l'environnement"},{"location":"doc/#entrer-dans-le-repertoire","text":"cd /data/Documentation git fetch git init git add . mkdocs gh-deploy -r https://github.com/HOUARIMouhssine/ahouari.github.io -b main","title":"Entrer dans le r\u00e9pertoire"},{"location":"doc/#creation-dun-service","text":"Pour que l'url du doc soit accessible en temps r\u00e9el ( ce qui veut dire sans sourcer l'environnement \u00e0 chaque fois et lancer le mkdocs serve), j'ai cr\u00e9e un fichier bash d\u00e9dier pour faire \u00e7a. [root@ah200 Documentation]# cat permanent_running.sh #!/bin/bash # Source your virtual environment source /data/python/bin/activate # Run mkdocs serve cd /data/Documentation && mkdocs serve Pour que cette partie soit permanente, j'ai cr\u00e9e un fichier daemon sous forme du service : [root@ah200 Documentation]# systemctl status mkdocs.service \u25cf mkdocs.service - MKDocs Service Loaded: loaded (/etc/systemd/system/mkdocs.service; disabled; preset: disabled) Active: active (running) since Mon 2024-02-12 16:50:31 UTC; 6 days ago Main PID: 902882 (permanent_runni) Tasks: 8 (limit: 203086) Memory: 33.5M CPU: 12min 40.005s CGroup: /system.slice/mkdocs.service \u251c\u2500902882 /bin/bash /data/Documentation/permanent_running.sh \u2514\u2500902883 /data/python/bin/python /data/python/bin/mkdocs serve [root@ah200 Documentation]# cat /etc/systemd/system/mkdocs.service [Unit] Description=MKDocs Service After=network.target [Service] Type=simple ExecStart=/bin/bash -c '/data/Documentation/permanent_running.sh' [Install] WantedBy=multi-user.target [root@ah200 Documentation]#","title":"Cr\u00e9ation d'un service"},{"location":"nfs/","text":"Welcome to MkDocs::NFS Pour toute la documentation, rendre visite \u00e0 mkdocs.org . Installation Sur la machine principale, il faut installer les packages suivants et les d\u00e9marrer via: dnf install nfs-utils -y systemctl enable nfs-server.service systemctl start nfs-server.service Configuration de NFS cat /etc/exports [root@ah200 etc]# cat /etc/exports /data/nfs *(rw,sync,no_subtree_check) et pour exporter ce folder on fait la commande suivant: exportfs -v /data/nfs <world>(sync,wdelay,hide,no_subtree_check,sec=sys,ro,secure,root_squash,no_all_squash) Pour \u00eatre s\u00fbr que le folder est export\u00e9 exportfs -s /data/nfs *(sync,wdelay,hide,no_subtree_check,sec=sys,ro,secure,root_squash,no_all_squash) Installation de provisionneur NFS dans le serveur principale helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ helm repo update helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --set nfs.server=ah200.ahouari \\ --set nfs.path=/data/nfs [root@ah200 nfs-psql]# kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE nfs-client cluster.local/nfs-subdir-external-provisioner Delete Immediate true 28d [root@ah200 nfs-psql]# Partie Client Pr\u00e9requis Il faut rajouter l'entr\u00e9e ah200.ahouari sur la partie /etc/hosts de chaque machine Cr\u00e9ation du dossier /data/nfs dans chaque worker/master mont\u00e9 la partition /data/nfs sur chaque worker dans le fichier /etc/fstab Il est pr\u00e9f\u00e9rable d'installer les packages suivant: dnf install nfs-utils nfs4-acl-tools -y Ajout de domaine name sur les workers Pour ce faire, j'ai b\u00e9n\u00e9fici\u00e9 du Playbook d'ansible kubespray pour lancer des commandes SHELL afin de rajouter la partie (python) [root@ah200 nfs] ansible kube_node -i inventory/mycluster/hosts.yaml -m shell -a \"echo 'ah200.ahouari:/data/nfs /mnt/nfs nfs defaults 0 0' >> /etc/fstab \" -b --become-user=root --become-method=sudo (python) [root@ah200 nfs] ansible kube_control_plane -i inventory/mycluster/hosts.yaml -m shell -a \"echo 'ah200.ahouari:/data/nfs /mnt/nfs nfs defaults 0 0' >> /etc/fstab \" -b --become-user=root --become-method=sudo Cr\u00e9ation du dossier (python) [root@ah200 nfs]ansible kube_control_plane -i inventory/mycluster/hosts.yaml -m command -a \"mkdir -p /mnt/nfs\" -b --become-user=root --become-method=sudo (python) [root@ah200 nfs]ansible kube_node -i inventory/mycluster/hosts.yaml -m command -a \"mkdir -p /mnt/nfs\" -b --become-user=root --become-method=sudo Montage /mnt/nfs sur ah200.ahouari:/data/nfs (python) [root@ah200 nfs] ansible kube_node -i inventory/mycluster/hosts.yaml -m shell -a \"mount -a\" -b --become-user=root --become-method=sudo (python) [root@ah200 nfs] ansible kube_control_plane -i inventory/mycluster/hosts.yaml -m shell -a \"mount -a\" -b --become-user=root --become-method=sudo (python) [root@ah200 nfs]ansible kube_node -i inventory/mycluster/hosts.yaml -m shell -a \"systemctl daemon-reload\" -b --become-user=root --become-method=sudo (python) [root@ah200 nfs]ansible kube_control_plane -i inventory/mycluster/hosts.yaml -m shell -a \"systemctl daemon-reload\" -b --become-user=root --become-method=sudo Example cr\u00e9ation BDD avec storage class NFS apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-storage provisioner: nfs-subdir-external-provisioner apiVersion: v1 kind: ConfigMap metadata: name: psql-itwl-dev-01-cm data: POSTGRES_DB: db POSTGRES_USER: ********* POSTGRES_PASSWORD: *********** PGDATA: /var/lib/postgresql/data/k8s --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: psql-itwl-dev-01-pvc spec: accessModes: - ReadWriteMany storageClassName: nfs-client resources: requests: storage: 1Gi apiVersion: apps/v1 kind: StatefulSet metadata: name: psql-itwl-dev-01 labels: app: psql ver: itwl-dev-01 spec: replicas: 1 selector: matchLabels: app: psql ver: itwl-dev-01 serviceName: \"itwl-dev-01\" template: #For the creation of the pod metadata: labels: app: psql ver: itwl-dev-01 spec: containers: - name: postgres image: postgres:latest imagePullPolicy: \"IfNotPresent\" ports: - containerPort: 5432 envFrom: - configMapRef: name: psql-itwl-dev-01-cm volumeMounts: - mountPath: /var/lib/postgresql/data name: pgdatavol volumes: - name: pgdatavol persistentVolumeClaim: claimName: psql-itwl-dev-01-pvc --- apiVersion: v1 kind: Service metadata: name: postgres-service-lb spec: type: LoadBalancer selector: app: psql ports: - name: psql port: 5432 targetPort: 5432 nodePort: 30101 protocol: TCP","title":"NFS"},{"location":"nfs/#welcome-to-mkdocsnfs","text":"Pour toute la documentation, rendre visite \u00e0 mkdocs.org .","title":"Welcome to MkDocs::NFS"},{"location":"nfs/#installation","text":"Sur la machine principale, il faut installer les packages suivants et les d\u00e9marrer via: dnf install nfs-utils -y systemctl enable nfs-server.service systemctl start nfs-server.service","title":"Installation"},{"location":"nfs/#configuration-de-nfs","text":"cat /etc/exports [root@ah200 etc]# cat /etc/exports /data/nfs *(rw,sync,no_subtree_check) et pour exporter ce folder on fait la commande suivant: exportfs -v /data/nfs <world>(sync,wdelay,hide,no_subtree_check,sec=sys,ro,secure,root_squash,no_all_squash) Pour \u00eatre s\u00fbr que le folder est export\u00e9 exportfs -s /data/nfs *(sync,wdelay,hide,no_subtree_check,sec=sys,ro,secure,root_squash,no_all_squash)","title":"Configuration de NFS"},{"location":"nfs/#installation-de-provisionneur-nfs-dans-le-serveur-principale","text":"helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ helm repo update helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --set nfs.server=ah200.ahouari \\ --set nfs.path=/data/nfs [root@ah200 nfs-psql]# kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE nfs-client cluster.local/nfs-subdir-external-provisioner Delete Immediate true 28d [root@ah200 nfs-psql]#","title":"Installation de provisionneur NFS dans le serveur principale"},{"location":"nfs/#partie-client","text":"Pr\u00e9requis Il faut rajouter l'entr\u00e9e ah200.ahouari sur la partie /etc/hosts de chaque machine Cr\u00e9ation du dossier /data/nfs dans chaque worker/master mont\u00e9 la partition /data/nfs sur chaque worker dans le fichier /etc/fstab Il est pr\u00e9f\u00e9rable d'installer les packages suivant: dnf install nfs-utils nfs4-acl-tools -y","title":"Partie Client"},{"location":"nfs/#ajout-de-domaine-name-sur-les-workers","text":"Pour ce faire, j'ai b\u00e9n\u00e9fici\u00e9 du Playbook d'ansible kubespray pour lancer des commandes SHELL afin de rajouter la partie (python) [root@ah200 nfs] ansible kube_node -i inventory/mycluster/hosts.yaml -m shell -a \"echo 'ah200.ahouari:/data/nfs /mnt/nfs nfs defaults 0 0' >> /etc/fstab \" -b --become-user=root --become-method=sudo (python) [root@ah200 nfs] ansible kube_control_plane -i inventory/mycluster/hosts.yaml -m shell -a \"echo 'ah200.ahouari:/data/nfs /mnt/nfs nfs defaults 0 0' >> /etc/fstab \" -b --become-user=root --become-method=sudo","title":"Ajout de domaine name sur les workers"},{"location":"nfs/#creation-du-dossier","text":"(python) [root@ah200 nfs]ansible kube_control_plane -i inventory/mycluster/hosts.yaml -m command -a \"mkdir -p /mnt/nfs\" -b --become-user=root --become-method=sudo (python) [root@ah200 nfs]ansible kube_node -i inventory/mycluster/hosts.yaml -m command -a \"mkdir -p /mnt/nfs\" -b --become-user=root --become-method=sudo","title":"Cr\u00e9ation du dossier"},{"location":"nfs/#montage-mntnfs-sur-ah200ahouaridatanfs","text":"(python) [root@ah200 nfs] ansible kube_node -i inventory/mycluster/hosts.yaml -m shell -a \"mount -a\" -b --become-user=root --become-method=sudo (python) [root@ah200 nfs] ansible kube_control_plane -i inventory/mycluster/hosts.yaml -m shell -a \"mount -a\" -b --become-user=root --become-method=sudo (python) [root@ah200 nfs]ansible kube_node -i inventory/mycluster/hosts.yaml -m shell -a \"systemctl daemon-reload\" -b --become-user=root --become-method=sudo (python) [root@ah200 nfs]ansible kube_control_plane -i inventory/mycluster/hosts.yaml -m shell -a \"systemctl daemon-reload\" -b --become-user=root --become-method=sudo","title":"Montage /mnt/nfs sur ah200.ahouari:/data/nfs"},{"location":"nfs/#example-creation-bdd-avec-storage-class-nfs","text":"apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-storage provisioner: nfs-subdir-external-provisioner apiVersion: v1 kind: ConfigMap metadata: name: psql-itwl-dev-01-cm data: POSTGRES_DB: db POSTGRES_USER: ********* POSTGRES_PASSWORD: *********** PGDATA: /var/lib/postgresql/data/k8s --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: psql-itwl-dev-01-pvc spec: accessModes: - ReadWriteMany storageClassName: nfs-client resources: requests: storage: 1Gi apiVersion: apps/v1 kind: StatefulSet metadata: name: psql-itwl-dev-01 labels: app: psql ver: itwl-dev-01 spec: replicas: 1 selector: matchLabels: app: psql ver: itwl-dev-01 serviceName: \"itwl-dev-01\" template: #For the creation of the pod metadata: labels: app: psql ver: itwl-dev-01 spec: containers: - name: postgres image: postgres:latest imagePullPolicy: \"IfNotPresent\" ports: - containerPort: 5432 envFrom: - configMapRef: name: psql-itwl-dev-01-cm volumeMounts: - mountPath: /var/lib/postgresql/data name: pgdatavol volumes: - name: pgdatavol persistentVolumeClaim: claimName: psql-itwl-dev-01-pvc --- apiVersion: v1 kind: Service metadata: name: postgres-service-lb spec: type: LoadBalancer selector: app: psql ports: - name: psql port: 5432 targetPort: 5432 nodePort: 30101 protocol: TCP","title":"Example cr\u00e9ation BDD avec storage class NFS"},{"location":"rook-ceph/","text":"Welcome to MkDocs::NFS Pour toute la documentation, rendre visite \u00e0 mkdocs.org . Afin de mettre en place un cluster Ceph il faut installer les deux composants: Ceph Operator Ceph Cluster Rook-ceph Operator On devait installer rook-ceph operator via Helm en utilisant la commande suivante: Ceph-Operator est d\u00e9di\u00e9 pour faire un watch de tout le cluster ceph `` helm repo add rook-release https://charts.rook.io/release helm install --create-namespace --namespace rook-ceph rook-ceph rook-release/rook-ceph -f values.yaml `` Le fichier values.yaml contient les valeurs suivant: csi: enableCephfsDriver: true enableRbdDriver: false Maintenant , dans le namespace rook-ceph on voit le pod Operator qui est en phase Running kubectl get pods -n rook-ceph rook-ceph-operator-5f4c57f467-2m6v5 1/1 Running 1 (4d16h ago) 7d23h Installation Sur le cluster kubernetes, nous devons installer les composants suivants: - rook-ceph operator - cluster rook-ceph dnf install nfs-utils -y systemctl enable nfs-server.service systemctl start nfs-server.service Configuration de NFS cat /etc/exports [root@ah200 etc]# cat /etc/exports /data/nfs *(rw,sync,no_subtree_check) et pour exporter ce folder on fait la commande suivant: exportfs -v /data/nfs <world>(sync,wdelay,hide,no_subtree_check,sec=sys,ro,secure,root_squash,no_all_squash) Pour \u00eatre s\u00fbr que le folder est export\u00e9 exportfs -s /data/nfs *(sync,wdelay,hide,no_subtree_check,sec=sys,ro,secure,root_squash,no_all_squash) Installation de provisionneur NFS dans le serveur principale helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ helm repo update helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --set nfs.server=ah200.ahouari \\ --set nfs.path=/data/nfs [root@ah200 nfs-psql]# kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE nfs-client cluster.local/nfs-subdir-external-provisioner Delete Immediate true 28d [root@ah200 nfs-psql]# Partie Client Pr\u00e9requis Il faut rajouter l'entr\u00e9e ah200.ahouari sur la partie /etc/hosts de chaque machine Cr\u00e9ation du dossier /data/nfs dans chaque worker/master mont\u00e9 la partition /data/nfs sur chaque worker dans le fichier /etc/fstab Il est pr\u00e9f\u00e9rable d'installer les packages suivant: dnf install nfs-utils nfs4-acl-tools -y Ajout de domaine name sur les workers Pour ce faire, j'ai b\u00e9n\u00e9fici\u00e9 du Playbook d'ansible kubespray pour lancer des commandes SHELL afin de rajouter la partie (python) [root@ah200 nfs] ansible kube_node -i inventory/mycluster/hosts.yaml -m shell -a \"echo 'ah200.ahouari:/data/nfs /mnt/nfs nfs defaults 0 0' >> /etc/fstab \" -b --become-user=root --become-method=sudo (python) [root@ah200 nfs] ansible kube_control_plane -i inventory/mycluster/hosts.yaml -m shell -a \"echo 'ah200.ahouari:/data/nfs /mnt/nfs nfs defaults 0 0' >> /etc/fstab \" -b --become-user=root --become-method=sudo Cr\u00e9ation du dossier (python) [root@ah200 nfs]ansible kube_control_plane -i inventory/mycluster/hosts.yaml -m command -a \"mkdir -p /mnt/nfs\" -b --become-user=root --become-method=sudo (python) [root@ah200 nfs]ansible kube_node -i inventory/mycluster/hosts.yaml -m command -a \"mkdir -p /mnt/nfs\" -b --become-user=root --become-method=sudo Montage /mnt/nfs sur ah200.ahouari:/data/nfs (python) [root@ah200 nfs] ansible kube_node -i inventory/mycluster/hosts.yaml -m shell -a \"mount -a\" -b --become-user=root --become-method=sudo (python) [root@ah200 nfs] ansible kube_control_plane -i inventory/mycluster/hosts.yaml -m shell -a \"mount -a\" -b --become-user=root --become-method=sudo (python) [root@ah200 nfs]ansible kube_node -i inventory/mycluster/hosts.yaml -m shell -a \"systemctl daemon-reload\" -b --become-user=root --become-method=sudo (python) [root@ah200 nfs]ansible kube_control_plane -i inventory/mycluster/hosts.yaml -m shell -a \"systemctl daemon-reload\" -b --become-user=root --become-method=sudo Example cr\u00e9ation BDD avec storage class NFS apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-storage provisioner: nfs-subdir-external-provisioner apiVersion: v1 kind: ConfigMap metadata: name: psql-itwl-dev-01-cm data: POSTGRES_DB: db POSTGRES_USER: ********* POSTGRES_PASSWORD: *********** PGDATA: /var/lib/postgresql/data/k8s --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: psql-itwl-dev-01-pvc spec: accessModes: - ReadWriteMany storageClassName: nfs-client resources: requests: storage: 1Gi apiVersion: apps/v1 kind: StatefulSet metadata: name: psql-itwl-dev-01 labels: app: psql ver: itwl-dev-01 spec: replicas: 1 selector: matchLabels: app: psql ver: itwl-dev-01 serviceName: \"itwl-dev-01\" template: #For the creation of the pod metadata: labels: app: psql ver: itwl-dev-01 spec: containers: - name: postgres image: postgres:latest imagePullPolicy: \"IfNotPresent\" ports: - containerPort: 5432 envFrom: - configMapRef: name: psql-itwl-dev-01-cm volumeMounts: - mountPath: /var/lib/postgresql/data name: pgdatavol volumes: - name: pgdatavol persistentVolumeClaim: claimName: psql-itwl-dev-01-pvc --- apiVersion: v1 kind: Service metadata: name: postgres-service-lb spec: type: LoadBalancer selector: app: psql ports: - name: psql port: 5432 targetPort: 5432 nodePort: 30101 protocol: TCP","title":"Welcome to MkDocs::NFS"},{"location":"rook-ceph/#welcome-to-mkdocsnfs","text":"Pour toute la documentation, rendre visite \u00e0 mkdocs.org . Afin de mettre en place un cluster Ceph il faut installer les deux composants: Ceph Operator Ceph Cluster","title":"Welcome to MkDocs::NFS"},{"location":"rook-ceph/#rook-ceph-operator","text":"On devait installer rook-ceph operator via Helm en utilisant la commande suivante: Ceph-Operator est d\u00e9di\u00e9 pour faire un watch de tout le cluster ceph `` helm repo add rook-release https://charts.rook.io/release helm install --create-namespace --namespace rook-ceph rook-ceph rook-release/rook-ceph -f values.yaml `` Le fichier values.yaml contient les valeurs suivant: csi: enableCephfsDriver: true enableRbdDriver: false Maintenant , dans le namespace rook-ceph on voit le pod Operator qui est en phase Running kubectl get pods -n rook-ceph rook-ceph-operator-5f4c57f467-2m6v5 1/1 Running 1 (4d16h ago) 7d23h","title":"Rook-ceph Operator"},{"location":"rook-ceph/#installation","text":"Sur le cluster kubernetes, nous devons installer les composants suivants: - rook-ceph operator - cluster rook-ceph dnf install nfs-utils -y systemctl enable nfs-server.service systemctl start nfs-server.service","title":"Installation"},{"location":"rook-ceph/#configuration-de-nfs","text":"cat /etc/exports [root@ah200 etc]# cat /etc/exports /data/nfs *(rw,sync,no_subtree_check) et pour exporter ce folder on fait la commande suivant: exportfs -v /data/nfs <world>(sync,wdelay,hide,no_subtree_check,sec=sys,ro,secure,root_squash,no_all_squash) Pour \u00eatre s\u00fbr que le folder est export\u00e9 exportfs -s /data/nfs *(sync,wdelay,hide,no_subtree_check,sec=sys,ro,secure,root_squash,no_all_squash)","title":"Configuration de NFS"},{"location":"rook-ceph/#installation-de-provisionneur-nfs-dans-le-serveur-principale","text":"helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ helm repo update helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --set nfs.server=ah200.ahouari \\ --set nfs.path=/data/nfs [root@ah200 nfs-psql]# kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE nfs-client cluster.local/nfs-subdir-external-provisioner Delete Immediate true 28d [root@ah200 nfs-psql]#","title":"Installation de provisionneur NFS dans le serveur principale"},{"location":"rook-ceph/#partie-client","text":"Pr\u00e9requis Il faut rajouter l'entr\u00e9e ah200.ahouari sur la partie /etc/hosts de chaque machine Cr\u00e9ation du dossier /data/nfs dans chaque worker/master mont\u00e9 la partition /data/nfs sur chaque worker dans le fichier /etc/fstab Il est pr\u00e9f\u00e9rable d'installer les packages suivant: dnf install nfs-utils nfs4-acl-tools -y","title":"Partie Client"},{"location":"rook-ceph/#ajout-de-domaine-name-sur-les-workers","text":"Pour ce faire, j'ai b\u00e9n\u00e9fici\u00e9 du Playbook d'ansible kubespray pour lancer des commandes SHELL afin de rajouter la partie (python) [root@ah200 nfs] ansible kube_node -i inventory/mycluster/hosts.yaml -m shell -a \"echo 'ah200.ahouari:/data/nfs /mnt/nfs nfs defaults 0 0' >> /etc/fstab \" -b --become-user=root --become-method=sudo (python) [root@ah200 nfs] ansible kube_control_plane -i inventory/mycluster/hosts.yaml -m shell -a \"echo 'ah200.ahouari:/data/nfs /mnt/nfs nfs defaults 0 0' >> /etc/fstab \" -b --become-user=root --become-method=sudo","title":"Ajout de domaine name sur les workers"},{"location":"rook-ceph/#creation-du-dossier","text":"(python) [root@ah200 nfs]ansible kube_control_plane -i inventory/mycluster/hosts.yaml -m command -a \"mkdir -p /mnt/nfs\" -b --become-user=root --become-method=sudo (python) [root@ah200 nfs]ansible kube_node -i inventory/mycluster/hosts.yaml -m command -a \"mkdir -p /mnt/nfs\" -b --become-user=root --become-method=sudo","title":"Cr\u00e9ation du dossier"},{"location":"rook-ceph/#montage-mntnfs-sur-ah200ahouaridatanfs","text":"(python) [root@ah200 nfs] ansible kube_node -i inventory/mycluster/hosts.yaml -m shell -a \"mount -a\" -b --become-user=root --become-method=sudo (python) [root@ah200 nfs] ansible kube_control_plane -i inventory/mycluster/hosts.yaml -m shell -a \"mount -a\" -b --become-user=root --become-method=sudo (python) [root@ah200 nfs]ansible kube_node -i inventory/mycluster/hosts.yaml -m shell -a \"systemctl daemon-reload\" -b --become-user=root --become-method=sudo (python) [root@ah200 nfs]ansible kube_control_plane -i inventory/mycluster/hosts.yaml -m shell -a \"systemctl daemon-reload\" -b --become-user=root --become-method=sudo","title":"Montage /mnt/nfs sur ah200.ahouari:/data/nfs"},{"location":"rook-ceph/#example-creation-bdd-avec-storage-class-nfs","text":"apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-storage provisioner: nfs-subdir-external-provisioner apiVersion: v1 kind: ConfigMap metadata: name: psql-itwl-dev-01-cm data: POSTGRES_DB: db POSTGRES_USER: ********* POSTGRES_PASSWORD: *********** PGDATA: /var/lib/postgresql/data/k8s --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: psql-itwl-dev-01-pvc spec: accessModes: - ReadWriteMany storageClassName: nfs-client resources: requests: storage: 1Gi apiVersion: apps/v1 kind: StatefulSet metadata: name: psql-itwl-dev-01 labels: app: psql ver: itwl-dev-01 spec: replicas: 1 selector: matchLabels: app: psql ver: itwl-dev-01 serviceName: \"itwl-dev-01\" template: #For the creation of the pod metadata: labels: app: psql ver: itwl-dev-01 spec: containers: - name: postgres image: postgres:latest imagePullPolicy: \"IfNotPresent\" ports: - containerPort: 5432 envFrom: - configMapRef: name: psql-itwl-dev-01-cm volumeMounts: - mountPath: /var/lib/postgresql/data name: pgdatavol volumes: - name: pgdatavol persistentVolumeClaim: claimName: psql-itwl-dev-01-pvc --- apiVersion: v1 kind: Service metadata: name: postgres-service-lb spec: type: LoadBalancer selector: app: psql ports: - name: psql port: 5432 targetPort: 5432 nodePort: 30101 protocol: TCP","title":"Example cr\u00e9ation BDD avec storage class NFS"},{"location":"vagrant/","text":"Welcome to MkDocs::Vagrant Pour toute la documentation, rendre visite \u00e0 mkdocs.org . Installation sudo dnf install -y dnf-plugins-core sudo dnf config-manager --add-repo https://rpm.releases.hashicorp.com/fedora/hashicorp.repo sudo dnf -y install vagrant on pourra apr\u00e8s voir sa version via vagrant --version Command utiles vagrant init : Pour initier vagrant file. vagrant global-status : Pour voir le status complet de tous les VMs cr\u00e9e si ils sont en mode running ou non. vagrant up <Vagrantfile> : Pour mettre en tension et d\u00e9marrer une VM aupr\u00e8s un fichier vagrant. vagrant reload : red\u00e9marrer une VM vagrant vagrant halt : stop une VM vagrant vagrant suspend : mettre une VM vagrant en mode suspende en gardant son \u00e9tat vagrant destroy <machine id> -f : supprimer une machine vagrant sans confirmation vagrant box list : pour r\u00e9cupirer tous les OS et les box install\u00e9s VagrantFile Vagrant.configure(\"2\") do |config| config.vm.box = \"almalinux/9\" config.vm.hostname = \"ah100\" config.vm.network :private_network, ip: \"192.168.0.10\", netmask:\"255.255.252.0\" config.vm.provider :virtualbox do |vb| vb.customize [ \"modifyvm\", :id, \"--cpuexecutioncap\", \"50\", \"--memory\", \"2048\", ] end end Un autre example de mettre en place une VM almalinux9 avec une configuration ssh qui permet: changer la partie PermitRootLogin to YES . changer le mot de passe de l'utilisateur root . Vagrant.configure(\"2\") do |config| config.vm.box = \"almalinux/9\" config.vm.hostname = \"ah101\" config.vm.network :private_network, ip: \"192.168.0.11\", netmask: \"255.255.252.0\" config.vm.provider :virtualbox do |vb| vb.customize [ \"modifyvm\", :id, \"--cpuexecutioncap\", \"50\", \"--memory\", \"2048\", ] end config.vm.provision \"shell\", inline: <<-SHELL # Set root password to 'your_password' echo 'root:your_password' | chpasswd # Enable PermitRootLogin in /etc/ssh/sshd_config sed -i 's/^#PermitRootLogin.*/PermitRootLogin yes/' /etc/ssh/sshd_config # Restart ssh service systemctl restart sshd SHELL end","title":"Vagrant"},{"location":"vagrant/#welcome-to-mkdocsvagrant","text":"Pour toute la documentation, rendre visite \u00e0 mkdocs.org .","title":"Welcome to MkDocs::Vagrant"},{"location":"vagrant/#installation","text":"sudo dnf install -y dnf-plugins-core sudo dnf config-manager --add-repo https://rpm.releases.hashicorp.com/fedora/hashicorp.repo sudo dnf -y install vagrant on pourra apr\u00e8s voir sa version via vagrant --version","title":"Installation"},{"location":"vagrant/#command-utiles","text":"vagrant init : Pour initier vagrant file. vagrant global-status : Pour voir le status complet de tous les VMs cr\u00e9e si ils sont en mode running ou non. vagrant up <Vagrantfile> : Pour mettre en tension et d\u00e9marrer une VM aupr\u00e8s un fichier vagrant. vagrant reload : red\u00e9marrer une VM vagrant vagrant halt : stop une VM vagrant vagrant suspend : mettre une VM vagrant en mode suspende en gardant son \u00e9tat vagrant destroy <machine id> -f : supprimer une machine vagrant sans confirmation vagrant box list : pour r\u00e9cupirer tous les OS et les box install\u00e9s","title":"Command utiles"},{"location":"vagrant/#vagrantfile","text":"Vagrant.configure(\"2\") do |config| config.vm.box = \"almalinux/9\" config.vm.hostname = \"ah100\" config.vm.network :private_network, ip: \"192.168.0.10\", netmask:\"255.255.252.0\" config.vm.provider :virtualbox do |vb| vb.customize [ \"modifyvm\", :id, \"--cpuexecutioncap\", \"50\", \"--memory\", \"2048\", ] end end Un autre example de mettre en place une VM almalinux9 avec une configuration ssh qui permet: changer la partie PermitRootLogin to YES . changer le mot de passe de l'utilisateur root . Vagrant.configure(\"2\") do |config| config.vm.box = \"almalinux/9\" config.vm.hostname = \"ah101\" config.vm.network :private_network, ip: \"192.168.0.11\", netmask: \"255.255.252.0\" config.vm.provider :virtualbox do |vb| vb.customize [ \"modifyvm\", :id, \"--cpuexecutioncap\", \"50\", \"--memory\", \"2048\", ] end config.vm.provision \"shell\", inline: <<-SHELL # Set root password to 'your_password' echo 'root:your_password' | chpasswd # Enable PermitRootLogin in /etc/ssh/sshd_config sed -i 's/^#PermitRootLogin.*/PermitRootLogin yes/' /etc/ssh/sshd_config # Restart ssh service systemctl restart sshd SHELL end","title":"VagrantFile"},{"location":"vbox/","text":"Welcome to MkDocs::VboxManage Pour toute la documentation, rendre visite \u00e0 mkdocs.org . VboxManage est la partie CLI pour Oracle VM virtualbox, il permet de cr\u00e9e des interfaces r\u00e9seaux, de les configurer, d\u00e9marrer ou l'arreter Dans notre example, il sera utiliser pour cr\u00e9er une interface r\u00e9seau sur le r\u00e9seau local dont lequel on va \u00e9couter Pour sp\u00e9cifier le machine folder des VMs via VBoxManage VBoxManage setproperty machinefolder /data/virtualbox/VM Cr\u00e9ation une inteface vboxmanage hostonlyif create VBoxManage hostonlyif ipconfig vboxnet0 --ip=192.168.0.1 --netmask=255.255.252.0 Cette inteface fait partie de la zone dns ahouari Configuration de Haproxy frontend http bind 5.196.93.233:80 use_backend %[req.hdr(Host),lower] frontend https bind 5.196.93.233:443 #ssl crt /etc/haproxy/cert/server.pem mode http option httplog use_backend %[req.hdr(Host),lower] Pour la partie backend (example: virtualbox) backend virtualbox.5.196.93.233.nip.io balance roundrobin server server_virtualbox_1 5.196.93.233:8080 check http Frontend : cela, veut dire que notre serveur \u00e9coute toujour sur l'IP 5.196.93.233 dans le port 80 use_backend %[req.hdr(Host),lower] : Cette partie explique qu'on \u00e9coute de chaque backend en basant sur son host header qui est case-insensitive et qui fait partie dans le dossier conf.d mode http : on \u00e9coute tous le traffic http sur SSL ( pas mode tcp) option httplog : cela veut dire qu'on active le mode log pour la partie http Si on avait un fichier pem, dans ce cas l\u00e0 , on va utiliser le trafic http over SSL pour chiffrer le traffic en mode loadbalancing * backend virtualbox.5.196.93.233.nip.io : le nom du backend utiliser, il revient vers un nom de nip.io qui est un DNS public donc la format est : nom.IP.nip.ip , le nom de ce backend est server_virtualbox_1 , ce backend va \u00e9couter sur le port 8080 avec l'IP public ( Image docker d\u00e8j\u00e0 cr\u00e9er) et configurer Docker-compose file version: \"3.5\" services: vbox_http: container_name: vbox_http image: joweisberg/phpvirtualbox restart: always ports: - 8080:80 environment: TZ=\"Europe/Paris\" SRV1_HOSTPORT=\"5.196.93.233:18083\" SRV1_NAME=\"Server1\" SRV1_USER=\"root\" SRV1_PW=\"*****\" CONF_browserRestrictFolders=\"/home,/usr/lib/virtualbox,\" CONF_noAuth=\"true\" Pour installer cette image de docker , il faut lancer la commande: docker-compose -f /data/docker-compose/phpvbox.yaml up -d Pour supprimer l'image docker docker-compose -f /data/docker-compose/phpvbox.yaml down Reset the password: Si on veut r\u00e9cup\u00e9rer le mot de passe de l'utilisateur admin; il faut se connecter sur le container du docker qui est phpvbox et renommer le fichier recovery.php-disabled vers recovery.php Apr\u00e9s il faut r\u00e9demmarer le service : vboxweb-service Restart DHCP Server apr\u00e8s avoir choisi le serveur dhcp via l'interface virtualbox, et des fois, il faut le r\u00e9demarrer pour prendre en compte le nouveau r\u00e9seau , pour cela il faut lancer : VBoxManage dhcpserver restart --interface=vboxnet1","title":"Virtualbox"},{"location":"vbox/#welcome-to-mkdocsvboxmanage","text":"Pour toute la documentation, rendre visite \u00e0 mkdocs.org . VboxManage est la partie CLI pour Oracle VM virtualbox, il permet de cr\u00e9e des interfaces r\u00e9seaux, de les configurer, d\u00e9marrer ou l'arreter Dans notre example, il sera utiliser pour cr\u00e9er une interface r\u00e9seau sur le r\u00e9seau local dont lequel on va \u00e9couter Pour sp\u00e9cifier le machine folder des VMs via VBoxManage VBoxManage setproperty machinefolder /data/virtualbox/VM","title":"Welcome to MkDocs::VboxManage"},{"location":"vbox/#creation-une-inteface","text":"vboxmanage hostonlyif create VBoxManage hostonlyif ipconfig vboxnet0 --ip=192.168.0.1 --netmask=255.255.252.0 Cette inteface fait partie de la zone dns ahouari","title":"Cr\u00e9ation une inteface"},{"location":"vbox/#configuration-de-haproxy","text":"frontend http bind 5.196.93.233:80 use_backend %[req.hdr(Host),lower] frontend https bind 5.196.93.233:443 #ssl crt /etc/haproxy/cert/server.pem mode http option httplog use_backend %[req.hdr(Host),lower] Pour la partie backend (example: virtualbox) backend virtualbox.5.196.93.233.nip.io balance roundrobin server server_virtualbox_1 5.196.93.233:8080 check http Frontend : cela, veut dire que notre serveur \u00e9coute toujour sur l'IP 5.196.93.233 dans le port 80 use_backend %[req.hdr(Host),lower] : Cette partie explique qu'on \u00e9coute de chaque backend en basant sur son host header qui est case-insensitive et qui fait partie dans le dossier conf.d mode http : on \u00e9coute tous le traffic http sur SSL ( pas mode tcp) option httplog : cela veut dire qu'on active le mode log pour la partie http Si on avait un fichier pem, dans ce cas l\u00e0 , on va utiliser le trafic http over SSL pour chiffrer le traffic en mode loadbalancing * backend virtualbox.5.196.93.233.nip.io : le nom du backend utiliser, il revient vers un nom de nip.io qui est un DNS public donc la format est : nom.IP.nip.ip , le nom de ce backend est server_virtualbox_1 , ce backend va \u00e9couter sur le port 8080 avec l'IP public ( Image docker d\u00e8j\u00e0 cr\u00e9er) et configurer","title":"Configuration de Haproxy"},{"location":"vbox/#docker-compose-file","text":"version: \"3.5\" services: vbox_http: container_name: vbox_http image: joweisberg/phpvirtualbox restart: always ports: - 8080:80 environment: TZ=\"Europe/Paris\" SRV1_HOSTPORT=\"5.196.93.233:18083\" SRV1_NAME=\"Server1\" SRV1_USER=\"root\" SRV1_PW=\"*****\" CONF_browserRestrictFolders=\"/home,/usr/lib/virtualbox,\" CONF_noAuth=\"true\" Pour installer cette image de docker , il faut lancer la commande: docker-compose -f /data/docker-compose/phpvbox.yaml up -d Pour supprimer l'image docker docker-compose -f /data/docker-compose/phpvbox.yaml down","title":"Docker-compose file"},{"location":"vbox/#reset-the-password","text":"Si on veut r\u00e9cup\u00e9rer le mot de passe de l'utilisateur admin; il faut se connecter sur le container du docker qui est phpvbox et renommer le fichier recovery.php-disabled vers recovery.php Apr\u00e9s il faut r\u00e9demmarer le service : vboxweb-service","title":"Reset the password:"},{"location":"vbox/#restart-dhcp-server","text":"apr\u00e8s avoir choisi le serveur dhcp via l'interface virtualbox, et des fois, il faut le r\u00e9demarrer pour prendre en compte le nouveau r\u00e9seau , pour cela il faut lancer : VBoxManage dhcpserver restart --interface=vboxnet1","title":"Restart DHCP Server"}]}